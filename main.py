from fastapi import FastAPI, HTTPException, Depends
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from openai import OpenAI
from sqlalchemy.orm import Session
from datetime import datetime, timedelta
import os
import requests
import json
from bs4 import BeautifulSoup
import re
import google.generativeai as genai

# Import database models
from database import get_db, init_db, Session as DBSession, Capture

# Import AI prompts
from focus_prompts import (
    SESSION_ANALYSIS_PROMPT,
    LEARNING_GUIDE_PROMPT,
    format_captures_for_analysis
)

# Try to load environment variables from .env file (ignore if file doesn't exist or can't be read)
try:
    from dotenv import load_dotenv
    load_dotenv(override=False)
except Exception as e:
    print(f"Note: Could not load .env file: {e}")
    print("Using environment variables from system instead.")

# Initialize FastAPI app
app = FastAPI(title="Chat API with Focus Catcher", version="1.0.0")

# Initialize database on startup
@app.on_event("startup")
def startup_event():
    init_db()
    print("âœ… Database initialized")

# Add CORS middleware to allow frontend to call the API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, replace with specific origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# OpenAI client will be initialized lazily
_client = None


def get_openai_client():
    """Get or create OpenAI client instance."""
    global _client
    if _client is None:
        api_key = os.getenv("SUPER_MIND_API_KEY")
        if not api_key:
            raise ValueError(
                "SUPER_MIND_API_KEY environment variable is not set. "
                "Please set it in your environment or create a .env file."
            )
        _client = OpenAI(
            api_key=api_key,
            base_url="https://space.ai-builders.com/backend/v1"
        )
    return _client


def get_gemini_model():
    """Get or create Gemini model instance."""
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError(
            "GOOGLE_API_KEY environment variable is not set. "
            "Please set it in your environment or create a .env file."
        )
    
    genai.configure(api_key=api_key)
    return genai.GenerativeModel('gemini-2.5-flash')  # ä½¿ç”¨æœ€æ–°æœ€å¿«çš„æ¨¡å‹


def print_message_history(messages: list):
    """
    Print the complete message history for debugging.
    Shows the full conversation flow including tool calls and results.
    """
    print("\n" + "="*80)
    print("ğŸ“‹ COMPLETE MESSAGE HISTORY (DEBUG)")
    print("="*80)
    
    for idx, msg in enumerate(messages, 1):
        role = msg.get("role", "unknown")
        
        print(f"\n[Message {idx}] Role: {role.upper()}")
        print("-" * 80)
        
        if role == "user":
            # User message
            content = msg.get("content", "")
            print(f"Content: {content}")
            
        elif role == "assistant":
            # Assistant message (may have tool_calls or content)
            content = msg.get("content")
            tool_calls = msg.get("tool_calls")
            
            if content:
                print(f"Content: {content}")
            else:
                print(f"Content: None")
            
            if tool_calls:
                print(f"\nTool Calls: {len(tool_calls)} call(s)")
                for tc_idx, tc in enumerate(tool_calls, 1):
                    func_name = tc.get("function", {}).get("name", "unknown")
                    func_args = tc.get("function", {}).get("arguments", "{}")
                    tc_id = tc.get("id", "unknown")
                    
                    print(f"  [{tc_idx}] Function: {func_name}")
                    print(f"      ID: {tc_id}")
                    print(f"      Arguments: {func_args}")
                    
        elif role == "tool":
            # Tool result
            tool_call_id = msg.get("tool_call_id", "unknown")
            content = msg.get("content", "")
            
            print(f"Tool Call ID: {tool_call_id}")
            
            # Try to parse and pretty-print JSON content
            try:
                content_obj = json.loads(content)
                
                # Check if it's an error
                if "error" in content_obj:
                    print(f"Result: ERROR - {content_obj['error']}")
                else:
                    # For successful results, show a summary
                    if "url" in content_obj:
                        # read_page result
                        print(f"Result Type: read_page")
                        print(f"  URL: {content_obj.get('url', 'N/A')}")
                        print(f"  Title: {content_obj.get('title', 'N/A')}")
                        print(f"  Content Length: {content_obj.get('length', 0)} chars")
                        print(f"  Content Preview: {content_obj.get('content', '')[:100]}...")
                    elif "queries" in content_obj:
                        # web_search result
                        print(f"Result Type: web_search")
                        queries = content_obj.get("queries", [])
                        print(f"  Number of queries: {len(queries)}")
                        if queries:
                            first_query = queries[0]
                            print(f"  First query keyword: {first_query.get('keyword', 'N/A')}")
                    else:
                        # Unknown format, show first 200 chars
                        print(f"Result: {content[:200]}...")
                        
            except (json.JSONDecodeError, Exception):
                # Not JSON or parsing failed, show raw content
                print(f"Result (raw): {content[:200]}...")
        
        elif role == "system":
            # System message
            content = msg.get("content", "")
            print(f"Content: {content}")
        
        else:
            # Unknown role
            print(f"Content: {msg}")
    
    print("\n" + "="*80)
    print("ğŸ“‹ END OF MESSAGE HISTORY")
    print("="*80 + "\n")


def web_search(query: str) -> dict:
    """
    Perform a web search using the internal search API.
    
    Args:
        query: The search query string
        
    Returns:
        dict: Search results from the API
        
    Raises:
        Exception: If the API call fails
    """
    api_key = os.getenv("SUPER_MIND_API_KEY")
    if not api_key:
        raise ValueError("SUPER_MIND_API_KEY environment variable is not set.")
    
    url = "https://space.ai-builders.com/backend/v1/search/"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    payload = {
        "keywords": [query],
        "max_results": 3
    }
    
    try:
        response = requests.post(url, json=payload, headers=headers)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        raise Exception(f"Web search API call failed: {str(e)}")


def read_page(url: str) -> dict:
    """
    Fetch a web page and extract its main text content.
    
    Args:
        url: The URL of the page to read
        
    Returns:
        dict: Contains the URL, title, and extracted text content
        
    Raises:
        Exception: If the page cannot be fetched or parsed
    """
    try:
        # Fetch the page
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # Parse HTML
        soup = BeautifulSoup(response.content, 'lxml')
        
        # Remove script and style elements
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
        
        # Get title
        title = soup.title.string if soup.title else "No title"
        
        # Extract text
        text = soup.get_text(separator='\n', strip=True)
        
        # Clean up whitespace
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = '\n'.join(chunk for chunk in chunks if chunk)
        
        # Limit text length to avoid overwhelming the LLM
        max_length = 8000
        if len(text) > max_length:
            text = text[:max_length] + "\n\n[Content truncated due to length...]"
        
        return {
            "url": url,
            "title": title,
            "content": text,
            "length": len(text)
        }
        
    except requests.exceptions.RequestException as e:
        raise Exception(f"Failed to fetch page: {str(e)}")
    except Exception as e:
        raise Exception(f"Failed to parse page: {str(e)}")


# Tool schema for LLM to understand available functions
TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "web_search",
            "description": "Search the web for current information. Use this when you need up-to-date information about events, facts, or topics that may have changed recently. Returns relevant search results from the internet.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The search query string. Be specific and use keywords that will return relevant results."
                    }
                },
                "required": ["query"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "read_page",
            "description": "Fetch and read the content of a specific web page. Use this when you have a URL and need to extract detailed information from that page. Returns the page title and main text content with scripts, styles, and navigation removed.",
            "parameters": {
                "type": "object",
                "properties": {
                    "url": {
                        "type": "string",
                        "description": "The full URL of the web page to read (must include http:// or https://)"
                    }
                },
                "required": ["url"]
            }
        }
    }
]


# Request model
class ChatRequest(BaseModel):
    user_message: str


# Response model
class ChatResponse(BaseModel):
    content: str
    tool_calls: list | None = None  # Optional field to show tool calls made by LLM


# ============================================================
# Focus Catcher Models
# ============================================================

class CaptureRequest(BaseModel):
    """Request model for capturing a learning focus point."""
    selected_text: str
    page_url: str
    page_title: str | None = None


class CaptureResponse(BaseModel):
    """Response model for capture endpoint."""
    success: bool
    capture_id: int
    session_id: int
    message: str


class SessionResponse(BaseModel):
    """Response model for session information."""
    id: int
    start_time: datetime
    end_time: datetime | None
    status: str
    capture_count: int


@app.get("/api")
async def root():
    """Root endpoint to verify the API is running."""
    return {"message": "Chat API is running. Use POST /chat to send messages."}


# Serve frontend
@app.get("/")
async def serve_frontend():
    """Serve the frontend HTML page."""
    return FileResponse("frontend/index.html")


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Chat endpoint with full Agentic Loop implementation.
    The LLM can call tools, receive results, and iterate up to max_turns times.
    
    Args:
        request: ChatRequest containing user_message field
        
    Returns:
        ChatResponse containing the assistant's final response and tool call history
    """
    max_turns = 10  # Maximum number of agent turns to prevent infinite loops (increased from 5)
    
    try:
        # Get OpenAI client
        client = get_openai_client()
        
        # Initialize conversation history
        messages = [
            {"role": "user", "content": request.user_message}
        ]
        
        # Track all tool calls made during the conversation
        all_tool_calls = []
        
        # Track consecutive empty responses
        consecutive_empty_responses = 0
        
        # Track consecutive tool-only turns (no text generation)
        consecutive_tool_turns = 0
        
        print(f"\n{'='*60}")
        print(f"[User] {request.user_message}")
        print(f"{'='*60}")
        
        # Agentic Loop: iterate up to max_turns
        for turn in range(max_turns):
            print(f"\n[Turn {turn + 1}/{max_turns}]")
            
            # Call LLM with current conversation history
            response = client.chat.completions.create(
                model="gpt-5",
                messages=messages,
                tools=TOOLS,
                tool_choice="auto"
            )
            
            # Extract the assistant's response
            message = response.choices[0].message
            
            # Add assistant's message to conversation history
            # Convert to dict format for messages array
            # Note: content can be None when tool_calls are present
            assistant_message = {
                "role": "assistant",
                "content": message.content if message.content else None
            }
            
            # Check if the model wants to call tools
            if message.tool_calls:
                # Increment consecutive tool turns counter
                consecutive_tool_turns += 1
                
                # Check if we've had too many consecutive tool calls
                if consecutive_tool_turns >= 5:
                    # LLM is stuck in a search loop - force it to generate an answer
                    print(f"[Agent] Warning: {consecutive_tool_turns} consecutive tool calls detected")
                    print(f"[Agent] Forcing answer generation to break the loop...")
                    
                    # Add a strong directive
                    messages.append({
                        "role": "system",
                        "content": "ä½ å·²ç»æœç´¢äº†è¶³å¤Ÿå¤šçš„ä¿¡æ¯ã€‚ç°åœ¨å¿…é¡»åœæ­¢æœç´¢ï¼ŒåŸºäºå·²è·å–çš„æ‰€æœ‰æœç´¢ç»“æœç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„å›ç­”ã€‚å³ä½¿æœç´¢ç»“æœä¸­æ²¡æœ‰ç›´æ¥ç­”æ¡ˆï¼Œä½ ä¹Ÿè¦æ€»ç»“é“¾æ¥ã€æ ‡é¢˜ç­‰ä¿¡æ¯ï¼Œæˆ–è€…å‘Šè¯‰ç”¨æˆ·ä½ æ‰¾åˆ°äº†å“ªäº›ç›¸å…³èµ„æºã€‚ä¸è¦å†è°ƒç”¨ä»»ä½•å·¥å…·ã€‚"
                    })
                    
                    # Call LLM without tools
                    try:
                        final_response = client.chat.completions.create(
                            model="gpt-5",
                            messages=messages,
                            tools=None,
                            temperature=0.7
                        )
                        
                        final_message = final_response.choices[0].message
                        final_answer = final_message.content or "æŠ±æ­‰ï¼Œè™½ç„¶æˆ‘è¿›è¡Œäº†å¤šæ¬¡æœç´¢ï¼Œä½†æ— æ³•ç”Ÿæˆæ»¡æ„çš„å›ç­”ã€‚å»ºè®®æ‚¨ç›´æ¥è®¿é—®ç›¸å…³æ–°é—»ç½‘ç«™è·å–æœ€æ–°ä¿¡æ¯ã€‚"
                        
                        print(f"[Agent] Forced Final Answer: {final_answer}")
                        print(f"{'='*60}\n")
                        
                        messages.append({
                            "role": "assistant",
                            "content": final_answer
                        })
                        
                        print_message_history(messages)
                        
                        return ChatResponse(
                            content=final_answer,
                            tool_calls=all_tool_calls if all_tool_calls else None
                        )
                    except Exception as e:
                        print(f"[Error] Failed to force answer: {e}")
                        # Continue to normal flow
                
                # Check if this is the last turn
                if turn == max_turns - 1:
                    # Last turn but LLM still wants to call tools
                    # Force it to generate an answer instead
                    print(f"[Agent] Warning: Last turn reached, but LLM wants to call {len(message.tool_calls)} tool(s)")
                    print(f"[Agent] Forcing final answer generation...")
                    
                    # Add a system message to force answer generation
                    messages.append({
                        "role": "system",
                        "content": "è¿™æ˜¯æœ€åä¸€è½®å¯¹è¯ã€‚è¯·åŸºäºå·²è·å–çš„ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œä¸è¦å†è°ƒç”¨å·¥å…·ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜å¹¶ç»™å‡ºéƒ¨åˆ†ç­”æ¡ˆã€‚"
                    })
                    
                    # Call LLM again without tools to force text generation
                    final_response = client.chat.completions.create(
                        model="gpt-5",
                        messages=messages,
                        tools=None,  # Disable tools
                        temperature=0.7
                    )
                    
                    final_message = final_response.choices[0].message
                    final_answer = final_message.content or "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå®Œæ•´çš„å›ç­”ã€‚è¯·å°è¯•ç®€åŒ–æ‚¨çš„é—®é¢˜ã€‚"
                    
                    print(f"[Agent] Forced Final Answer: {final_answer}")
                    print(f"{'='*60}\n")
                    
                    # Add final message to history
                    messages.append({
                        "role": "assistant",
                        "content": final_answer
                    })
                    
                    print_message_history(messages)
                    
                    return ChatResponse(
                        content=final_answer,
                        tool_calls=all_tool_calls if all_tool_calls else None
                    )
                
                print(f"[Agent] Decided to call {len(message.tool_calls)} tool(s)")
                print(f"[Agent] Consecutive tool-only turns: {consecutive_tool_turns}")
                
                # Reset empty response counter (we got tool calls)
                consecutive_empty_responses = 0
                
                # Add tool_calls to assistant message
                # Fix: Ensure content is empty string instead of None to avoid API errors
                if assistant_message["content"] is None:
                    assistant_message["content"] = ""
                
                assistant_message["tool_calls"] = [
                    {
                        "id": tc.id,
                        "type": "function",
                        "function": {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments
                        }
                    }
                    for tc in message.tool_calls
                ]
                messages.append(assistant_message)
                
                # Execute each tool call
                for tool_call in message.tool_calls:
                    function_name = tool_call.function.name
                    function_args = json.loads(tool_call.function.arguments)
                    
                    print(f"[Agent] Calling tool: '{function_name}'")
                    print(f"[Agent] Arguments: {function_args}")
                    
                    # Track this tool call
                    all_tool_calls.append({
                        "id": tool_call.id,
                        "function": function_name,
                        "arguments": function_args
                    })
                    
                    # Execute the tool
                    try:
                        if function_name == "web_search":
                            query = function_args.get("query", "")
                            tool_result = web_search(query)
                            
                            # Format the result for display
                            result_str = json.dumps(tool_result, ensure_ascii=False, indent=2)
                            print(f"[System] Tool Output: {result_str[:200]}..." if len(result_str) > 200 else f"[System] Tool Output: {result_str}")
                            
                            # Add tool result to conversation history
                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "content": result_str
                            })
                            
                        elif function_name == "read_page":
                            url = function_args.get("url", "")
                            tool_result = read_page(url)
                            
                            # Format the result for display
                            result_str = json.dumps(tool_result, ensure_ascii=False, indent=2)
                            print(f"[System] Tool Output (read_page):")
                            print(f"[System]   URL: {tool_result.get('url', 'N/A')}")
                            print(f"[System]   Title: {tool_result.get('title', 'N/A')}")
                            print(f"[System]   Content length: {tool_result.get('length', 0)} characters")
                            print(f"[System]   Preview: {tool_result.get('content', '')[:150]}...")
                            
                            # Add tool result to conversation history
                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "content": result_str
                            })
                            
                        else:
                            error_msg = f"Unknown tool: {function_name}"
                            print(f"[System] Error: {error_msg}")
                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "content": json.dumps({"error": error_msg})
                            })
                    
                    except Exception as e:
                        error_msg = f"Tool execution failed: {str(e)}"
                        print(f"[System] Error: {error_msg}")
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": json.dumps({"error": error_msg})
                        })
                
                # Continue to next turn to let LLM process the tool results
                continue
            
            # No tool calls - check if we have a final answer
            elif message.content:
                # LLM has provided final answer with content
                messages.append(assistant_message)
                
                # Reset counters
                consecutive_empty_responses = 0
                consecutive_tool_turns = 0
                
                final_answer = message.content
                print(f"[Agent] Final Answer: {final_answer}")
                print(f"{'='*60}\n")
                
                # DEBUG: Print complete message history before returning
                print_message_history(messages)
                
                return ChatResponse(
                    content=final_answer,
                    tool_calls=all_tool_calls if all_tool_calls else None
                )
            
            else:
                # No tool calls AND no content - this is unusual
                # This might happen if the LLM returns an empty response
                
                consecutive_empty_responses += 1
                
                print(f"[Agent] Warning: Received response with no tool calls and no content")
                print(f"[Agent] Consecutive empty responses: {consecutive_empty_responses}")
                print(f"[Agent] Turn {turn + 1}/{max_turns}: Attempting to recover...")
                
                # If we've had 2+ consecutive empty responses, force a final answer
                if consecutive_empty_responses >= 2 or turn == max_turns - 1:
                    print(f"[Agent] Too many empty responses or last turn - forcing final answer...")
                    
                    # Add a strong directive to generate an answer
                    messages.append({
                        "role": "system",
                        "content": "ä½ å¿…é¡»ç«‹å³ç”Ÿæˆä¸€ä¸ªå›ç­”ã€‚è¯·åŸºäºä¹‹å‰è·å–çš„ä»»ä½•ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœæ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·è¯šå®åœ°å‘Šè¯‰ç”¨æˆ·ä½ æ— æ³•è·å–å‡†ç¡®ä¿¡æ¯ï¼Œä½†å°½é‡æä¾›ä¸€äº›ç›¸å…³å»ºè®®ã€‚ä¸è¦è¿”å›ç©ºå“åº”ã€‚"
                    })
                    
                    # Call LLM one more time without tools to force text generation
                    try:
                        final_response = client.chat.completions.create(
                            model="gpt-5",
                            messages=messages,
                            tools=None,  # Disable tools
                            temperature=0.7
                        )
                        
                        final_message = final_response.choices[0].message
                        final_answer = final_message.content or "æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†å›°éš¾ã€‚æˆ‘å·²ç»å°è¯•æœç´¢ç›¸å…³ä¿¡æ¯ï¼Œä½†æ— æ³•ç”Ÿæˆå®Œæ•´çš„å›ç­”ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ï¼Œæˆ–å°†å…¶åˆ†è§£æˆæ›´ç®€å•çš„éƒ¨åˆ†ã€‚"
                        
                        print(f"[Agent] Forced Final Answer: {final_answer}")
                        print(f"{'='*60}\n")
                        
                        messages.append({
                            "role": "assistant",
                            "content": final_answer
                        })
                        
                        print_message_history(messages)
                        
                        return ChatResponse(
                            content=final_answer,
                            tool_calls=all_tool_calls if all_tool_calls else None
                        )
                    except Exception as e:
                        print(f"[Error] Failed to force final answer: {e}")
                        final_answer = "æŠ±æ­‰ï¼Œæˆ‘åœ¨ç”Ÿæˆå›ç­”æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ï¼Œæˆ–å°†é—®é¢˜åˆ†è§£æˆæ›´ç®€å•çš„éƒ¨åˆ†ã€‚"
                        
                        print_message_history(messages)
                        
                        return ChatResponse(
                            content=final_answer,
                            tool_calls=all_tool_calls if all_tool_calls else None
                        )
                
                # First empty response - add a guidance prompt
                messages.append({
                    "role": "system",
                    "content": "è¯·åŸºäºå·²è·å–çš„æœç´¢ç»“æœï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„å›ç­”ã€‚å¦‚æœæœç´¢ç»“æœä¸­åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·æå–å¹¶æ€»ç»“ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜å¹¶ç»™å‡ºéƒ¨åˆ†ç­”æ¡ˆã€‚"
                })
                
                print(f"[Agent] Added guidance prompt, retrying...")
                
                # Continue to next turn with the guidance
                continue
        
        # Max turns reached without final answer
        print(f"[System] Max turns ({max_turns}) reached")
        print(f"{'='*60}\n")
        
        # DEBUG: Print complete message history before returning
        print_message_history(messages)
        
        return ChatResponse(
            content="I apologize, but I've reached the maximum number of steps. Please try rephrasing your question.",
            tool_calls=all_tool_calls if all_tool_calls else None
        )
        
    except ValueError as e:
        # Handle missing API key
        print(f"[Error] {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=str(e)
        )
    except Exception as e:
        # Handle any errors that occur during the API call
        print(f"[Error] {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error in agentic loop: {str(e)}"
        )


# ============================================================
# Focus Catcher Endpoints
# ============================================================

def detect_topic_shift(new_text: str, recent_captures: list, db: Session) -> tuple[bool, str]:
    """
    Use AI to detect if the new capture represents a topic shift.
    
    Args:
        new_text: The newly captured text
        recent_captures: List of recent Capture objects (last 3-5)
        db: Database session
    
    Returns:
        (topic_shifted: bool, new_topic: str)
    """
    if len(recent_captures) < 3:
        # Not enough data to determine topic shift
        return False, ""
    
    try:
        # Prepare context from recent captures
        recent_texts = "\n\n".join([
            f"æ•æ‰ {i+1}: {cap.selected_text[:200]}"
            for i, cap in enumerate(recent_captures[:3])
        ])
        
        # Create prompt for topic detection
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªå­¦ä¹ ä¸»é¢˜è¯†åˆ«åŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„å­¦ä¹ æ•æ‰å†…å®¹ï¼Œåˆ¤æ–­æ–°æ•æ‰æ˜¯å¦ä¸ä¹‹å‰çš„ä¸»é¢˜ç›¸å…³ã€‚

æœ€è¿‘çš„æ•æ‰å†…å®¹ï¼š
{recent_texts}

æ–°çš„æ•æ‰å†…å®¹ï¼š
{new_text[:200]}

è¯·åˆ†æï¼š
1. æ–°æ•æ‰çš„ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
2. å®ƒä¸ä¹‹å‰çš„æ•æ‰æ˜¯å¦å±äºåŒä¸€å­¦ä¹ ä¸»é¢˜ï¼Ÿ

åˆ¤æ–­æ ‡å‡†ï¼š
- å¦‚æœæ˜¯åŒä¸€æŠ€æœ¯æ ˆã€åŒä¸€é—®é¢˜é¢†åŸŸã€æˆ–ç›¸å…³æ¦‚å¿µ â†’ ç›¸å…³
- å¦‚æœæ˜¯å®Œå…¨ä¸åŒçš„é¢†åŸŸã€æŠ€æœ¯æˆ–è¯é¢˜ â†’ ä¸ç›¸å…³

è¯·ç”¨ JSON æ ¼å¼å›ç­”ï¼š
{{
  "related": true/false,
  "new_topic": "æ–°ä¸»é¢˜çš„ç®€çŸ­æè¿°ï¼ˆå¦‚æœä¸ç›¸å…³ï¼‰",
  "confidence": 0.0-1.0,
  "reason": "åˆ¤æ–­ç†ç”±"
}}"""

        # Call Gemini for fast analysis
        gemini_model = get_gemini_model()
        response = gemini_model.generate_content(
            prompt,
            generation_config={
                "temperature": 0.3,
                "response_mime_type": "application/json"
            }
        )
        
        result = json.loads(response.text)
        
        is_related = result.get("related", True)
        new_topic = result.get("new_topic", "")
        confidence = result.get("confidence", 0.5)
        reason = result.get("reason", "")
        
        print(f"[Topic Detection] Related: {is_related}, Confidence: {confidence:.2f}")
        print(f"[Topic Detection] Reason: {reason}")
        
        if not is_related and confidence > 0.6:
            print(f"[Topic Detection] ğŸ”„ Topic shift detected: {new_topic}")
            return True, new_topic
        
        return False, ""
        
    except Exception as e:
        print(f"[Topic Detection] Error: {e}")
        # On error, assume no topic shift (fail safe)
        return False, ""


def get_or_create_active_session(db: Session, new_capture_text: str = None) -> tuple[DBSession, bool, str]:
    """
    Get the current active session or create a new one based on topic detection.
    
    Args:
        db: Database session
        new_capture_text: The text being captured (for topic detection)
    
    Returns:
        (session, topic_shifted, new_topic)
    """
    # Find the most recent active session
    latest_session = db.query(DBSession).filter(
        DBSession.status == "active"
    ).order_by(DBSession.start_time.desc()).first()
    
    # If no active session exists, create one
    if not latest_session:
        new_session = DBSession(
            start_time=datetime.utcnow(),
            status="active",
            core_goal="æ–°å­¦ä¹ ä¼šè¯"
        )
        db.add(new_session)
        db.commit()
        db.refresh(new_session)
        print(f"[Focus Catcher] Created first session: {new_session.id}")
        return new_session, False, ""
    
    # If we have a new capture text, check for topic shift
    if new_capture_text:
        # Get recent captures from the current session
        recent_captures = db.query(Capture).filter(
            Capture.session_id == latest_session.id
        ).order_by(Capture.timestamp.desc()).limit(5).all()
        
        # Detect topic shift
        topic_shifted, new_topic = detect_topic_shift(new_capture_text, recent_captures, db)
        
        if topic_shifted:
            # Mark current session as completed
            latest_session.status = "completed"
            latest_session.end_time = datetime.utcnow()
            db.commit()
            
            # Create new session for the new topic
            new_session = DBSession(
                start_time=datetime.utcnow(),
                status="active",
                core_goal=new_topic
            )
            db.add(new_session)
            db.commit()
            db.refresh(new_session)
            
            print(f"[Focus Catcher] ğŸ”„ Topic shift! Created new session #{new_session.id}: {new_topic}")
            return new_session, True, new_topic
    
    # Continue with current session
    return latest_session, False, ""


@app.post("/api/focus/capture", response_model=CaptureResponse)
async def capture_focus(request: CaptureRequest, db: Session = Depends(get_db)):
    """
    Capture a learning focus point with intelligent topic detection.
    This endpoint uses AI to detect topic shifts and automatically create new sessions.
    
    Args:
        request: CaptureRequest containing selected_text, page_url, page_title
        db: Database session
    
    Returns:
        CaptureResponse with success status, IDs, and topic shift information
    """
    try:
        start_time = datetime.utcnow()
        
        # Get or create active session with topic detection
        session, topic_shifted, new_topic = get_or_create_active_session(db, request.selected_text)
        
        # Create capture record
        capture = Capture(
            session_id=session.id,
            selected_text=request.selected_text,
            page_url=request.page_url,
            page_title=request.page_title,
            timestamp=datetime.utcnow()
        )
        
        db.add(capture)
        db.commit()
        db.refresh(capture)
        
        # Update session capture count
        capture_count = db.query(Capture).filter(
            Capture.session_id == session.id
        ).count()
        
        # Calculate response time
        response_time = (datetime.utcnow() - start_time).total_seconds() * 1000
        
        # Build response message
        if topic_shifted:
            message = f"ğŸ”„ æ£€æµ‹åˆ°æ–°ä¸»é¢˜ï¼š{new_topic}ï¼Œå·²åˆ›å»ºæ–°ä¼šè¯ #{session.id}"
        else:
            message = f"âœ… å·²æ•æ‰åˆ°ä¼šè¯ #{session.id}"
        
        print(f"[Focus Catcher] Captured focus point #{capture.id} in session #{session.id}")
        print(f"[Focus Catcher] Response time: {response_time:.2f}ms")
        print(f"[Focus Catcher] Text preview: {request.selected_text[:100]}...")
        
        # Check if we should trigger batch analysis (5-10 captures)
        if capture_count >= 5:
            print(f"[Focus Catcher] ğŸ¯ Session has {capture_count} captures - ready for AI analysis")
        
        return CaptureResponse(
            success=True,
            capture_id=capture.id,
            session_id=session.id,
            message=message
        )
        
    except Exception as e:
        print(f"[Focus Catcher] Error: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Failed to capture: {str(e)}"
        )


@app.get("/api/focus/sessions")
async def get_sessions(db: Session = Depends(get_db)):
    """
    Get all learning sessions with capture counts.
    """
    try:
        sessions = db.query(DBSession).order_by(DBSession.start_time.desc()).all()
        
        result = []
        for session in sessions:
            capture_count = db.query(Capture).filter(Capture.session_id == session.id).count()
            result.append({
                "id": session.id,
                "start_time": session.start_time.isoformat(),
                "end_time": session.end_time.isoformat() if session.end_time else None,
                "status": session.status,
                "capture_count": capture_count
            })
        
        return {"sessions": result}
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get sessions: {str(e)}"
        )


@app.get("/api/focus/captures/{session_id}")
async def get_captures(session_id: int, db: Session = Depends(get_db)):
    """
    Get all captures for a specific session.
    """
    try:
        captures = db.query(Capture).filter(
            Capture.session_id == session_id
        ).order_by(Capture.timestamp.asc()).all()
        
        result = []
        for capture in captures:
            result.append({
                "id": capture.id,
                "selected_text": capture.selected_text,
                "page_url": capture.page_url,
                "page_title": capture.page_title,
                "timestamp": capture.timestamp.isoformat(),
                "focus_point": capture.focus_point,
                "content_type": capture.content_type,
                "suggested_action": capture.suggested_action
            })
        
        return {"captures": result}
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get captures: {str(e)}"
        )


@app.delete("/api/focus/sessions/{session_id}")
async def delete_session(session_id: int, db: Session = Depends(get_db)):
    """
    Delete a learning session and all its captures.
    
    Args:
        session_id: The session ID to delete
        db: Database session
    
    Returns:
        Success message
    """
    try:
        # Check if session exists
        session = db.query(DBSession).filter(DBSession.id == session_id).first()
        if not session:
            raise HTTPException(status_code=404, detail=f"Session {session_id} not found")
        
        # Get capture count before deletion
        capture_count = db.query(Capture).filter(Capture.session_id == session_id).count()
        
        # Delete all captures for this session
        db.query(Capture).filter(Capture.session_id == session_id).delete()
        
        # Delete the session
        db.delete(session)
        db.commit()
        
        print(f"[Focus Catcher] ğŸ—‘ï¸ Deleted session #{session_id} with {capture_count} captures")
        
        return {
            "success": True,
            "message": f"Session #{session_id} and {capture_count} captures deleted successfully"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        print(f"[Focus Catcher] Error deleting session: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to delete session: {str(e)}"
        )


@app.post("/api/focus/analyze/{session_id}")
def analyze_session(session_id: int, db: Session = Depends(get_db)):
    """
    Analyze a learning session using AI.
    Generate insights about learning goals, main threads, branches, and action guide.
    
    Args:
        session_id: The session ID to analyze
        db: Database session
    
    Returns:
        Analysis results including core goal, main thread, branches, and action guide
    """
    try:
        # Get session
        session = db.query(DBSession).filter(DBSession.id == session_id).first()
        if not session:
            raise HTTPException(status_code=404, detail=f"Session {session_id} not found")
        
        # Get all captures for this session
        captures = db.query(Capture).filter(
            Capture.session_id == session_id
        ).order_by(Capture.timestamp.asc()).all()
        
        if len(captures) == 0:
            raise HTTPException(status_code=400, detail="Session has no captures to analyze")
        
        print(f"\n{'='*60}")
        print(f"[Focus Catcher] ğŸ¤– Starting AI analysis for session #{session_id}")
        print(f"[Focus Catcher] Captures to analyze: {len(captures)}")
        print(f"{'='*60}\n")
        
        # Format captures for analysis
        captures_data = []
        for capture in captures:
            captures_data.append({
                'id': capture.id,
                'timestamp': capture.timestamp.isoformat(),
                'selected_text': capture.selected_text,
                'page_title': capture.page_title,
                'page_url': capture.page_url
            })
        
        captures_text = format_captures_for_analysis(captures_data)
        
        # Prepare analysis prompt
        analysis_prompt = SESSION_ANALYSIS_PROMPT.format(
            session_id=session_id,
            start_time=session.start_time.isoformat(),
            capture_count=len(captures),
            captures_list=captures_text
        )
        
        # Call LLM for analysis
        USE_MOCK_DATA = False  # ä½¿ç”¨ Google Gemini API
        
        if USE_MOCK_DATA:
            print("[Focus Catcher] ğŸ§ª Using mock data for testing...")
            
            # ä½¿ç”¨å›ºå®šçš„æµ‹è¯•æ•°æ®
            analysis_json = {
                "core_goal": "å­¦ä¹ å’ŒéªŒè¯ Focus Catcher çš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬æ•æ‰é€Ÿåº¦ã€ä¼šè¯åˆ†ç»„å’Œ AI åˆ†æèƒ½åŠ›",
                "main_thread": [
                    "éªŒè¯æ•æ‰åŠŸèƒ½çš„å“åº”é€Ÿåº¦æ˜¯å¦è¾¾æ ‡ï¼ˆç›®æ ‡ < 200msï¼‰",
                    "æµ‹è¯• 15 åˆ†é’Ÿä¼šè¯åˆ†ç»„é€»è¾‘æ˜¯å¦åˆç†",
                    "éªŒè¯æ‰¹é‡åˆ†æè§¦å‘æœºåˆ¶ï¼ˆ5-10 æ¡è§¦å‘ï¼‰"
                ],
                "branches": [
                    "æ¢ç´¢ Chrome æ’ä»¶çš„å®ç°æ–¹æ¡ˆ",
                    "ç ”ç©¶ AI Prompt çš„ä¼˜åŒ–ç­–ç•¥",
                    "æ€è€ƒçœŸå®å­¦ä¹ åœºæ™¯çš„åº”ç”¨"
                ],
                "understood": [
                    "æ•æ‰åŠŸèƒ½å·¥ä½œæ­£å¸¸ï¼Œå“åº”æ—¶é—´å¹³å‡ 14msï¼Œè¿œè¶…é¢„æœŸ",
                    "ä¼šè¯è‡ªåŠ¨åˆ†ç»„åŠŸèƒ½æ­£å¸¸ï¼Œ15 åˆ†é’Ÿè§„åˆ™ç”Ÿæ•ˆ",
                    "æ‰¹é‡åˆ†æè§¦å‘æç¤ºå·²æ­£ç¡®æ˜¾ç¤º"
                ],
                "unclear": [
                    "AI åˆ†æçš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§å¦‚ä½•",
                    "åœ¨çœŸå®å­¦ä¹ åœºæ™¯ä¸­çš„ä½“éªŒå¦‚ä½•",
                    "Chrome æ’ä»¶çš„å¿«æ·é”®æ˜¯å¦çœŸçš„å¤Ÿä¸æ»‘"
                ],
                "action_guide": [
                    "å®Œæˆ AI åˆ†æåŠŸèƒ½çš„ LLM è°ƒç”¨ä¿®å¤",
                    "å¼€å‘ Chrome æ’ä»¶åŸå‹ï¼ŒéªŒè¯å¿«æ·é”®ä½“éªŒ",
                    "åœ¨çœŸå®å­¦ä¹ åœºæ™¯ä¸­æµ‹è¯•æ•æ‰åŠŸèƒ½",
                    "æ”¶é›†ä½¿ç”¨åé¦ˆï¼Œè¿­ä»£ä¼˜åŒ–äº§å“"
                ],
                "learning_pattern": "ç³»ç»ŸåŒ–æµ‹è¯•é©±åŠ¨ - ä½ é‡‡ç”¨äº†é€æ­¥éªŒè¯æ¯ä¸ªåŠŸèƒ½æ¨¡å—çš„æ–¹æ³•ï¼Œè¿™ç¡®ä¿äº†äº§å“çš„ç¨³å®šæ€§å’Œå¯é æ€§"
            }
            
            print("[Focus Catcher] âœ… Mock data loaded")
            
        else:
            # ä½¿ç”¨ Google Gemini API
            model = get_gemini_model()
            
            print("[Focus Catcher] ğŸ§  Calling Gemini for deep analysis...")
            
            # å‡†å¤‡æ•æ‰å†…å®¹æ‘˜è¦
            captures_summary = "\n".join([
                f"{idx+1}. {c['selected_text'][:200]}" 
                for idx, c in enumerate(captures_data)
            ])
            
            user_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå­¦ä¹ è·¯å¾„åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹ {len(captures_data)} æ¡å­¦ä¹ æ•æ‰è®°å½•ï¼Œè¯†åˆ«ç”¨æˆ·çš„å­¦ä¹ ç›®æ ‡å’Œæ¨¡å¼ã€‚

å­¦ä¹ æ•æ‰è®°å½•ï¼š
{captures_summary}

è¯·è¿”å› JSON æ ¼å¼çš„åˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- core_goal: æ ¸å¿ƒå­¦ä¹ ç›®æ ‡ï¼ˆå­—ç¬¦ä¸²ï¼Œç®€æ´æè¿°ç”¨æˆ·åœ¨å­¦ä»€ä¹ˆï¼‰
- main_thread: ä¸»çº¿é—®é¢˜ï¼ˆå­—ç¬¦ä¸²æ•°ç»„ï¼Œ2-3ä¸ªæ ¸å¿ƒå…³æ³¨ç‚¹ï¼‰
- branches: åˆ†æ”¯é—®é¢˜ï¼ˆå­—ç¬¦ä¸²æ•°ç»„ï¼Œ1-3ä¸ªå»¶ä¼¸æˆ–ç›¸å…³é—®é¢˜ï¼‰
- understood: å·²ç»ç†è§£çš„éƒ¨åˆ†ï¼ˆå­—ç¬¦ä¸²æ•°ç»„ï¼Œ1-3ä¸ªè¦ç‚¹ï¼‰
- unclear: è¿˜éœ€è¦å¼„æ¸…æ¥šçš„é—®é¢˜ï¼ˆå­—ç¬¦ä¸²æ•°ç»„ï¼Œ1-3ä¸ªé—®é¢˜ï¼‰
- action_guide: ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®ï¼ˆå­—ç¬¦ä¸²æ•°ç»„ï¼Œ3-5ä¸ªå…·ä½“å¯æ‰§è¡Œçš„æ­¥éª¤ï¼‰
- learning_pattern: å­¦ä¹ æ¨¡å¼è§‚å¯Ÿï¼ˆå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ï¼šæ·±åº¦ä¼˜å…ˆã€å¹¿åº¦ä¼˜å…ˆã€é—®é¢˜é©±åŠ¨ç­‰ï¼‰

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
            
            print(f"[Focus Catcher] Prompt length: {len(user_prompt)} chars")
            
            try:
                response = model.generate_content(user_prompt)
                analysis_result = response.text
                
                print(f"[Focus Catcher] âœ… Gemini response received")
                print(f"[Focus Catcher] Response length: {len(analysis_result)} chars")
                print(f"[Focus Catcher] Response preview:")
                print("="*80)
                print(analysis_result[:500] if analysis_result else "(None or empty)")
                print("="*80)
                
                # Parse JSON result
                try:
                    # å°è¯•ç›´æ¥è§£æ
                    analysis_json = json.loads(analysis_result)
                    print("[Focus Catcher] âœ… JSON parsed successfully")
                    
                    # æ¸…ç† HTML æ ‡ç­¾ï¼ˆå¦‚ <br>ã€<br/>ï¼‰
                    def clean_html_tags(text):
                        """ç§»é™¤ HTML æ ‡ç­¾ï¼Œä¿æŒçº¯æ–‡æœ¬"""
                        if isinstance(text, str):
                            # ç§»é™¤ <br>ã€<br/>ã€<br />
                            text = re.sub(r'<br\s*/?>',  '\n', text, flags=re.IGNORECASE)
                            # ç§»é™¤å…¶ä»– HTML æ ‡ç­¾
                            text = re.sub(r'<[^>]+>', '', text)
                            # æ¸…ç†å¤šä½™çš„ç©ºç™½
                            text = re.sub(r'\n\s*\n', '\n', text)
                            text = text.strip()
                        return text
                    
                    # é€’å½’æ¸…ç† JSON ä¸­çš„æ‰€æœ‰å­—ç¬¦ä¸²
                    def clean_json_strings(obj):
                        if isinstance(obj, dict):
                            return {k: clean_json_strings(v) for k, v in obj.items()}
                        elif isinstance(obj, list):
                            return [clean_json_strings(item) for item in obj]
                        elif isinstance(obj, str):
                            return clean_html_tags(obj)
                        return obj
                    
                    analysis_json = clean_json_strings(analysis_json)
                    print("[Focus Catcher] âœ… HTML tags cleaned")
                except json.JSONDecodeError as e:
                    print(f"[Focus Catcher] âŒ JSON parse error: {e}")
                    print(f"[Focus Catcher] Attempting regex extraction...")
                    
                    # å°è¯•æå– JSONï¼ˆGemini å¯èƒ½ä¼šåœ¨ JSON å‰ååŠ æ–‡å­—ï¼‰
                    json_match = re.search(r'\{.*\}', analysis_result, re.DOTALL)
                    if json_match:
                        try:
                            analysis_json = json.loads(json_match.group())
                            print("[Focus Catcher] âœ… JSON extracted via regex")
                        except:
                            raise ValueError(f"Failed to parse extracted JSON: {json_match.group()[:200]}")
                    else:
                        raise ValueError(f"No JSON found in response: {analysis_result[:200]}")
                        
            except Exception as e:
                print(f"[Focus Catcher] âŒ Gemini API error: {e}")
                raise ValueError(f"Gemini API call failed: {str(e)}")
        
        # Generate user-friendly learning guide
        print("[Focus Catcher] ğŸ“ Generating learning guide...")
        
        if USE_MOCK_DATA:
            # ä½¿ç”¨å›ºå®šçš„å­¦ä¹ æŒ‡å—
            learning_guide = """# ğŸ¯ ä½ çš„å­¦ä¹ ä¸»çº¿

ä½ æ­£åœ¨ç³»ç»ŸåŒ–åœ°æµ‹è¯•å’ŒéªŒè¯ **Focus Catcher** çš„æ ¸å¿ƒåŠŸèƒ½ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸æ‰å®çš„æ–¹æ³•ï¼

## ğŸ“š ä½ æ­£åœ¨æ¢ç´¢çš„é—®é¢˜

### ä¸»è¦é—®é¢˜
â€¢ æ•æ‰åŠŸèƒ½çš„å“åº”é€Ÿåº¦æ˜¯å¦è¾¾æ ‡ï¼ˆç›®æ ‡ < 200msï¼‰
â€¢ 15 åˆ†é’Ÿä¼šè¯åˆ†ç»„é€»è¾‘æ˜¯å¦åˆç†
â€¢ æ‰¹é‡åˆ†æè§¦å‘æœºåˆ¶ï¼ˆ5-10 æ¡è§¦å‘ï¼‰æ˜¯å¦æ­£å¸¸

### å»¶ä¼¸é—®é¢˜
â€¢ Chrome æ’ä»¶çš„å®ç°æ–¹æ¡ˆ
â€¢ AI Prompt çš„ä¼˜åŒ–ç­–ç•¥
â€¢ çœŸå®å­¦ä¹ åœºæ™¯çš„åº”ç”¨

## âœ… ä½ å·²ç»ç†è§£çš„éƒ¨åˆ†

â€¢ **æ•æ‰é€Ÿåº¦è¶…é¢„æœŸ** - å“åº”æ—¶é—´å¹³å‡ 14msï¼Œè¿œä½äº 200ms ç›®æ ‡
â€¢ **ä¼šè¯åˆ†ç»„æ­£å¸¸** - 15 åˆ†é’Ÿè§„åˆ™ç”Ÿæ•ˆï¼Œè‡ªåŠ¨åˆ›å»ºæ–°ä¼šè¯
â€¢ **è§¦å‘æœºåˆ¶æ­£ç¡®** - è¾¾åˆ° 5 æ¡æ—¶æ­£ç¡®æ˜¾ç¤ºåˆ†ææŒ‰é’®

## ğŸ¤” è¿˜éœ€è¦å¼„æ¸…æ¥šçš„

- [ ] AI åˆ†æçš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§
- [ ] çœŸå®å­¦ä¹ åœºæ™¯ä¸­çš„ä½“éªŒ
- [ ] Chrome æ’ä»¶å¿«æ·é”®æ˜¯å¦è¶³å¤Ÿä¸æ»‘

## ğŸš€ å»ºè®®çš„ä¸‹ä¸€æ­¥

1. **ä¿®å¤ AI åˆ†æåŠŸèƒ½** - è§£å†³ LLM è°ƒç”¨çš„ bugï¼Œå°è¯•ç®€åŒ– Prompt æˆ–ä½¿ç”¨ JSON mode
2. **å¼€å‘ Chrome æ’ä»¶åŸå‹** - éªŒè¯å¿«æ·é”®ä½“éªŒï¼Œåœ¨çœŸå®ç½‘é¡µä¸­æµ‹è¯•
3. **çœŸå®åœºæ™¯æµ‹è¯•** - åœ¨æ—¥å¸¸å­¦ä¹ ä¸­ä½¿ç”¨ï¼Œæ”¶é›†çœŸå®åé¦ˆ
4. **è¿­ä»£ä¼˜åŒ–** - æ ¹æ®åé¦ˆæ”¹è¿›äº§å“

## ğŸ’¡ å­¦ä¹ æ¨¡å¼è§‚å¯Ÿ

ä½ é‡‡ç”¨äº†**ç³»ç»ŸåŒ–æµ‹è¯•é©±åŠ¨**çš„æ–¹æ³• - é€æ­¥éªŒè¯æ¯ä¸ªåŠŸèƒ½æ¨¡å—ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†äº§å“çš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚ç»§ç»­ä¿æŒè¿™ç§ä¸¥è°¨çš„æ€åº¦ï¼

---

**åŠ æ²¹ï¼ä½ å·²ç»å®Œæˆäº† 90% çš„æ ¸å¿ƒåŠŸèƒ½ã€‚** ğŸ‰
"""
            print("[Focus Catcher] âœ… Mock learning guide loaded")
            
        else:
            # ç›´æ¥ä½¿ç”¨åˆ†æç»“æœç”Ÿæˆç®€æ´çš„å›é¡¾æŒ‡å—ï¼ˆä¸è°ƒç”¨ Geminiï¼‰
            print("[Focus Catcher] ğŸ“ Generating learning guide...")
            
            try:
                # æ ¼å¼åŒ–åŸæ–‡å†…å®¹
                original_texts = []
                for idx, capture in enumerate(captures_data, 1):
                    # æˆªå–å‰ 150 å­—ç¬¦ï¼Œå¦‚æœå¤ªé•¿åˆ™æ·»åŠ çœç•¥å·
                    text = capture['selected_text']
                    if len(text) > 150:
                        text = text[:150] + '...'
                    original_texts.append(f"{idx}. {text}")
                
                # ä½¿ç”¨çº¯æ–‡æœ¬æ¨¡æ¿ï¼Œä¸åŒ…å«ä»»ä½• Markdown ç¬¦å·
                learning_guide = f"""ğŸ¯ æ ¸å¿ƒä¸»é¢˜
{analysis_json.get('core_goal', 'æ­£åœ¨å­¦ä¹ ä¸­...')}

ğŸ“š å…³é”®ä¿¡æ¯ç‚¹
{chr(10).join([f'{idx+1}. {item}' for idx, item in enumerate(analysis_json.get('main_thread', []))])}

ğŸ”— å†…å®¹è„‰ç»œ
{chr(10).join(analysis_json.get('branches', []))}

âœ… å·²è¦†ç›–çš„å†…å®¹
{chr(10).join([f'{idx+1}. {item}' for idx, item in enumerate(analysis_json.get('understood', []))])}

â“ å¯èƒ½éœ€è¦è¿›ä¸€æ­¥æŸ¥é˜…
{chr(10).join([f'{idx+1}. {item}' for idx, item in enumerate(analysis_json.get('unclear', []))])}

ğŸ’¡ å›é¡¾å»ºè®®
{chr(10).join([f'{idx+1}. {item}' for idx, item in enumerate(analysis_json.get('action_guide', []))])}

ğŸ“Š å†…å®¹ç‰¹ç‚¹
{analysis_json.get('learning_pattern', 'ç»§ç»­ä¿æŒå­¦ä¹ çš„èŠ‚å¥')}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ åŸæ–‡å›é¡¾ï¼ˆå…± {len(captures_data)} æ¡æ•æ‰ï¼‰

{chr(10).join(original_texts)}"""
                
                print("[Focus Catcher] âœ… Learning guide generated (with original texts)")
                print(f"[Focus Catcher] Guide length: {len(learning_guide)} chars")
                print(f"[Focus Catcher] Included {len(captures_data)} original captures")
            except Exception as e:
                print(f"[Focus Catcher] âŒ Guide generation error: {e}")
                # å¤‡ç”¨ï¼šä½¿ç”¨ç®€å•çš„çº¯æ–‡æœ¬æ¨¡æ¿
                try:
                    original_texts = []
                    for idx, capture in enumerate(captures_data, 1):
                        text = capture['selected_text']
                        if len(text) > 150:
                            text = text[:150] + '...'
                        original_texts.append(f"{idx}. {text}")
                except:
                    original_texts = ["(æ— æ³•åŠ è½½åŸæ–‡)"]
                
                learning_guide = f"""ğŸ¯ æ ¸å¿ƒä¸»é¢˜
{analysis_json.get('core_goal', 'æ­£åœ¨å­¦ä¹ ä¸­...')}

ğŸ“š å…³é”®ä¿¡æ¯ç‚¹
{chr(10).join([f'{idx+1}. {item}' for idx, item in enumerate(analysis_json.get('main_thread', []))])}

âœ… å·²è¦†ç›–çš„å†…å®¹
{chr(10).join([f'{idx+1}. {item}' for idx, item in enumerate(analysis_json.get('understood', []))])}

â“ å¯èƒ½éœ€è¦è¿›ä¸€æ­¥æŸ¥é˜…
{chr(10).join([f'{idx+1}. {item}' for idx, item in enumerate(analysis_json.get('unclear', []))])}

ğŸ’¡ å›é¡¾å»ºè®®
{chr(10).join([f'{idx+1}. {item}' for idx, item in enumerate(analysis_json.get('action_guide', []))])}

ğŸ“Š å†…å®¹ç‰¹ç‚¹
{analysis_json.get('learning_pattern', 'ç»§ç»­ä¿æŒå­¦ä¹ çš„èŠ‚å¥')}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ åŸæ–‡å›é¡¾ï¼ˆå…± {len(captures_data)} æ¡æ•æ‰ï¼‰

{chr(10).join(original_texts)}"""
        
        print("[Focus Catcher] âœ… Learning guide generated")
        
        # Update session with analysis results
        session.core_goal = analysis_json.get('core_goal', '')
        session.main_thread = json.dumps(analysis_json.get('main_thread', []), ensure_ascii=False)
        session.branches = json.dumps(analysis_json.get('branches', []), ensure_ascii=False)
        session.action_guide = learning_guide
        session.status = 'completed'  # Mark session as analyzed
        
        db.commit()
        
        print(f"[Focus Catcher] ğŸ’¾ Analysis saved to database")
        print(f"{'='*60}\n")
        
        # Return results
        return {
            "success": True,
            "session_id": session_id,
            "analysis": {
                "core_goal": analysis_json.get('core_goal', ''),
                "main_thread": analysis_json.get('main_thread', []),
                "branches": analysis_json.get('branches', []),
                "understood": analysis_json.get('understood', []),
                "unclear": analysis_json.get('unclear', []),
                "action_guide": analysis_json.get('action_guide', []),
                "learning_pattern": analysis_json.get('learning_pattern', '')
            },
            "learning_guide": learning_guide,
            "capture_count": len(captures)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"[Focus Catcher] âŒ Analysis failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to analyze session: {str(e)}"
        )


# Mount static files (CSS, JS) - must be done after all routes are defined
app.mount("/", StaticFiles(directory="frontend", html=True), name="frontend")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

